---
title: "Stat 537: Homework 2"
author: "Doug Anderson and Brandon Fenton"
date: "Due Wednesday, Jan 27 at end of day"
output: html_document
---

1. _Use the dredge function from the R package MuMIn on your "full" model (say something like fm1) to generate AICs for all possible models._

```{r p1_a, echo=F, comment="", message=F, warning=F}
require(Rlab)
data(golf)
require(MuMIn)
options(na.action = "na.fail")

full.lm <- lm(score ~ distance + accur + putts, data = golf)
models <- dredge(full.lm, rank=AIC)
models

```

2. _For AICs, we usually scale the models using Delta-AIC as a difference in distance from the true model as compared to the top AIC model. Differences of moe than 2 units are considered to be "large" and strong support for a model that is 2 units closer to the truth. This is reported in the delta column in the dredge output. Use these ideas to discuss support for your top model versus others. Do not use the word "significant" in your discussion._

3. _Also compare support for the model with no predictors to your top model (if it isn't your top model) on AICs. In the model summary() for your top model, here is a result that parallels this comparison. Report that result and discuss whether they lead you to the same suggestions about these two models._

```{r p3_a, echo=F, comment="", message=F, warning=F}

summary(get.models(models,1)[[1]])

```

4. _Now we will switch to using cross-validation for these same sorts of comparisons. Read JWHT page 175 to 181. Write R code to calculate the LOOCV error for a model with a common mean for all observations (so the model is lm(y~1,data=?)). Do not use any functions that directly calculate the LOOCV (see below). Report the R code and your estimate of CV$_{(n)}$ for this model._ 
```{r p4_a, echo=F, comment="", message=F, warning=F}
n <- dim(golf)[1]
CVn <- mean((golf$score-mean(golf$score))^2)


```

5. _Based on JWHT result 5.2, you can also use the diagonal from the hat matrix to do LOOCV or you can mimic what you might have done in the previous questions that invlovled deleting each observation and producing the required fitted value. The $h_{ii}$'s are available in R using influence(fm1)$hat. Now write and report your own code to find the LOOCV for your top ranked, second ranked, and worst ranked models from the AIC model selection. Discuss these comparisons (note: these are on the squared units of your response variable and there are no rules of thumb for differences). Also note wheher they generally agree with the AIC results or not._

```{r p5_a, echo=F, comment="", message=F, warning=F}
require(cvTools)

LOOfolds <- cvFolds(n=n, K=n, R=1)
cvLm(full.lm, cost=mspe, folds=LOOfolds)
```

6. _Read JWHT 5.1.3. Now we can consider k-fold cross-validation for these same comparisons. Use 5-fold cross-validation to estimate the mean-squared prediction error for your top model. Modify the cvFolds object to get 5-fold CV. Compare this result to the estimate from LOOCV._

```{r p6_a, echo=F, comment="", message=F, warning=F}
set.seed(777)
k5folds <- cvFolds(n=n, K=5, R=1)
cvLm(full.lm, cost=mspe, folds=k5folds)
```

7. _Now, change the seed and re-run all the lines of code from 6 for one of the models. What is 5-fold CV and why does this estimated error change? Why does LOOCV not change if you change the seed?_

```{r p7_a, echo=F, comment="", message=F, warning=F}

```

## R Code Appendix:

```{r a1, ref.label='p1_a', eval=F}
```
